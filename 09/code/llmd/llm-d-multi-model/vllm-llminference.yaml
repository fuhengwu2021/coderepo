# vLLM LLMInferenceService for Llama-3.2-1B-Instruct
# Using llm-d Custom Resource Definition
apiVersion: llm-d.ai/v1alpha1
kind: LLMInferenceService
metadata:
  name: vllm-llama-32-1b
  labels:
    app: vllm
    model: llama-32-1b
spec:
  # Model configuration
  model:
    name: meta-llama/Llama-3.2-1B-Instruct
    path: meta-llama/Llama-3.2-1B-Instruct
  
  # Inference server configuration
  inferenceServer:
    type: vllm
    image: vllm/vllm-openai:v0.12.0
    command:
      - python3
      - -m
      - vllm.entrypoints.openai.api_server
    args:
      - --model
      - meta-llama/Llama-3.2-1B-Instruct
      - --host
      - "0.0.0.0"
      - --port
      - "8000"
      - --tensor-parallel-size
      - "1"
      - --gpu-memory-utilization
      - "0.9"
      - --trust-remote-code
  
  # Resource requirements
  resources:
    requests:
      nvidia.com/gpu: 1
      memory: 6Gi
    limits:
      nvidia.com/gpu: 1
      memory: 8Gi
  
  # Environment variables
  env:
    - name: HF_HOME
      value: "/models/hub"
    - name: TRANSFORMERS_CACHE
      value: "/models/hub"
    - name: HF_HUB_CACHE
      value: "/models/hub"
    - name: HF_TOKEN
      valueFrom:
        secretKeyRef:
          name: hf-token-secret
          key: token
  
  # Volume mounts
  volumes:
    - name: models
      hostPath:
        path: /models
        type: Directory
  volumeMounts:
    - name: models
      mountPath: /models
  
  # Service configuration
  service:
    type: ClusterIP
    port: 8000
  
  # Health checks
  livenessProbe:
    httpGet:
      path: /health
      port: 8000
    initialDelaySeconds: 60
    periodSeconds: 10
    timeoutSeconds: 5
  readinessProbe:
    httpGet:
      path: /health
      port: 8000
    initialDelaySeconds: 60
    periodSeconds: 5
    timeoutSeconds: 3
  
  # Runtime class for GPU support
  runtimeClassName: nvidia
