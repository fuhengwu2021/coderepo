multinode: false

modelArtifacts:
  uri: "hf://Qwen/Qwen2.5-0.5B-Instruct"  # HuggingFace model path
  name: "Qwen/Qwen2.5-0.5B-Instruct"
  size: 2Gi
  authSecretName: ""  # Qwen2.5-0.5B-Instruct is not gated, no token needed

routing:
  servicePort: 8000
  proxy:
    image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.4.0-rc.1
    connector: nixlv2
    secure: false

decode:
  create: true
  replicas: 1
  extraConfig:
    runtimeClassName: nvidia  # Required for k3d GPU access
    nodeSelector:
      kubernetes.io/hostname: k3d-llmd-multiengine-agent-0  # Schedule to agent-0 (same as vLLM)
    securityContext:
      fsGroup: 0  # Set group to root so pod can write to /models/hub
  containers:
  - name: "sglang"
    image: lmsysorg/sglang:v0.5.6.post2-runtime  # Use official SGLang image
    modelCommand: custom  # Use custom command for SGLang
    command:
      - python3
      - -m
      - sglang.launch_server
    args:
      - --model-path
      - Qwen/Qwen2.5-0.5B-Instruct
      - --host
      - "0.0.0.0"
      - --port
      - "8000"
      - --mem-fraction-static
      - "0.1"  # Use 10% of GPU memory
    env:
      - name: NVIDIA_VISIBLE_DEVICES
        value: "1"  # Only expose GPU 1 to container (host GPU 1)
      - name: CUDA_VISIBLE_DEVICES
        value: "0"  # Use GPU 0 within container (which maps to host GPU 1)
      - name: CUDA_DEVICE_ORDER
        value: "PCI_BUS_ID"  # Ensure consistent GPU ordering
      - name: HF_HOME
        value: "/models/hub"
      - name: TRANSFORMERS_CACHE
        value: "/models/hub"
      - name: HF_HUB_CACHE
        value: "/models/hub"
    resources:
      limits:
        nvidia.com/gpu: "1"
        memory: 8Gi
      requests:
        nvidia.com/gpu: "1"
        memory: 4Gi
    mountModelVolume: true  # Use default model volume mount for HuggingFace models

prefill:
  create: false
