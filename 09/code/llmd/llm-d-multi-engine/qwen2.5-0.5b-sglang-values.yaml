multinode: false
modelArtifacts:
  uri: "hf://Qwen/Qwen2.5-0.5B-Instruct" # HuggingFace model path
  name: "Qwen/Qwen2.5-0.5B-Instruct"
  size: 2Gi
  authSecretName: "" # Qwen2.5-0.5B-Instruct is not gated, no token needed
routing:
  servicePort: 54321
  proxy:
    image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.4.0-rc.1
    connector: sglang # Use SGLang connector instead of nixlv2
    secure: false
decode:
  create: true
  replicas: 1
  extraConfig:
    runtimeClassName: nvidia # Required for k3d GPU access
    nodeSelector:
      kubernetes.io/hostname: k3d-llmd-multiengine-agent-0 # Schedule to agent-0 (same as vLLM)
    securityContext:
      fsGroup: 0 # Set group to root so pod can write to /models/hub
  containers:
    - name: "sglang"
      image: lmsysorg/sglang:v0.5.6.post2-runtime # Use official SGLang image
      modelCommand: custom  # Use custom command (sglangServe not supported yet)
      command:
        - python3
        - -m
        - sglang.launch_server
      args:
        - --model-path
        - Qwen/Qwen2.5-0.5B-Instruct
        - --host
        - "0.0.0.0"
        - --port
        - "54321"  # Use port 54321 (same as routing-proxy servicePort, but in different container)
        - --mem-fraction-static
        - "0.2"  # Use 20% of GPU memory (increased from 0.1 to fix memory issues)
        - --enable-metrics  # Enable Prometheus metrics endpoint
      env:
        - name: NVIDIA_VISIBLE_DEVICES
          value: "1"
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        - name: CUDA_DEVICE_ORDER
          value: PCI_BUS_ID
      resources:
        limits:
          nvidia.com/gpu: "1"
          memory: 8Gi
        requests:
          nvidia.com/gpu: "1"
          memory: 4Gi
      mountModelVolume: true # Use default model volume mount for HuggingFace models
prefill:
  create: false
