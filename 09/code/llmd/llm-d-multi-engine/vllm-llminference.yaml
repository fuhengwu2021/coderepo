# vLLM LLMInferenceService for Qwen2.5-0.5B-Instruct
# Using llm-d Custom Resource Definition
apiVersion: llm-d.ai/v1alpha1
kind: LLMInferenceService
metadata:
  name: vllm-qwen2-5-0-5b
  labels:
    app: vllm
    model: qwen2-5-0-5b
spec:
  # Model configuration
  model:
    name: Qwen/Qwen2.5-0.5B-Instruct
    path: Qwen/Qwen2.5-0.5B-Instruct
  
  # Inference server configuration
  inferenceServer:
    type: vllm
    image: vllm/vllm-openai:v0.12.0
    command:
      - python3
      - -m
      - vllm.entrypoints.openai.api_server
    args:
      - --model
      - Qwen/Qwen2.5-0.5B-Instruct
      - --host
      - "0.0.0.0"
      - --port
      - "8000"
      - --tensor-parallel-size
      - "1"
      - --gpu-memory-utilization
      - "0.1"
      # Note: Qwen2.5-0.5B-Instruct doesn't require --trust-remote-code
  
  # Resource requirements
  resources:
    requests:
      nvidia.com/gpu: 1
      memory: 6Gi
    limits:
      nvidia.com/gpu: 1
      memory: 8Gi
  
  # Environment variables
  env:
    - name: NVIDIA_VISIBLE_DEVICES
      value: "0"  # Only expose GPU 0 to container (host GPU 0)
    - name: CUDA_VISIBLE_DEVICES
      value: "0"  # Use GPU 0 within container (which maps to host GPU 0)
    - name: CUDA_DEVICE_ORDER
      value: "PCI_BUS_ID"  # Ensure consistent GPU ordering
    - name: HF_HOME
      value: "/models/hub"
    - name: TRANSFORMERS_CACHE
      value: "/models/hub"
    - name: HF_HUB_CACHE
      value: "/models/hub"
    # Note: Qwen2.5-0.5B-Instruct is not gated, so HF_TOKEN is optional
    # Uncomment if you need to access other gated models
    # - name: HF_TOKEN
    #   valueFrom:
    #     secretKeyRef:
    #       name: hf-token-secret
    #       key: token
  
  # Volume mounts
  volumes:
    - name: models
      hostPath:
        path: /models
        type: Directory
  volumeMounts:
    - name: models
      mountPath: /models
  
  # Service configuration
  service:
    type: ClusterIP
    port: 8000
  
  # Health checks
  livenessProbe:
    httpGet:
      path: /health
      port: 8000
    initialDelaySeconds: 60
    periodSeconds: 10
    timeoutSeconds: 5
  readinessProbe:
    httpGet:
      path: /health
      port: 8000
    initialDelaySeconds: 60
    periodSeconds: 5
    timeoutSeconds: 3
  
  # Runtime class for GPU support
  runtimeClassName: nvidia
