# Slurm configuration for single-node setup with 2 GPUs
# Replace HOSTNAME with your actual hostname
# You can use: $(hostname -s) or $HOSTNAME when generating this file
# Note: Slurm config files don't expand variables, so use the setup script or substitute manually
ClusterName=distributed-ai
SlurmctldHost=HOSTNAME

# Authentication
AuthType=auth/munge
CryptoType=crypto/munge

# Paths and files - using %n for per-node substitution
# Replace USERNAME with your actual username (e.g., $USER or specific username)
# Replace SLURM_PREFIX with your Slurm installation prefix (e.g., /opt/slurm or $HOME/slurm)
SlurmctldPidFile=SLURM_PREFIX/var/state/slurmctld.pid
SlurmctldPort=6817
SlurmdPidFile=SLURM_PREFIX/var/spool/%n/slurmd.pid
SlurmdPort=6818
SlurmdSpoolDir=SLURM_PREFIX/var/spool/%n
StateSaveLocation=SLURM_PREFIX/var/state

# Logging
SlurmctldDebug=info
SlurmctldLogFile=SLURM_PREFIX/var/log/slurmctld.log
SlurmdDebug=info
SlurmdLogFile=SLURM_PREFIX/var/log/slurmd.%n.log

# Process tracking and task binding
# Using proctrack/pgid for virtual nodes (doesn't require matching hardware topology)
# proctrack/linux requires exact hardware match, which fails with virtual nodes
ProctrackType=proctrack/pgid
TaskPlugin=task/affinity

# Scheduling
SchedulerType=sched/backfill
SelectType=select/cons_tres
SelectTypeParameters=CR_Core_Memory

# Resource limits
DefMemPerCPU=4000
MaxMemPerCPU=0

# Timeouts
SlurmctldTimeout=120
SlurmdTimeout=300
InactiveLimit=0
MinJobAge=300
KillWait=30
Waittime=0

# User - replace USERNAME with your actual username
SlurmUser=USERNAME
SlurmdUser=USERNAME

# MPI support for distributed training
# Using pmi2 - PMIx plugin requires Slurm recompile with --with-pmix
# To use PMIx: recompile Slurm with: ./configure ... --with-pmix
MpiDefault=pmi2

# GRES
GresTypes=gpu

# 2 virtual nodes for GPU allocation
# Each node uses different port for multiple slurmd support
# Replace HOSTNAME with your actual hostname (e.g., $(hostname) or specific hostname)
# Adjust CPUs and RealMemory based on your hardware
# For virtual nodes, let Slurm auto-detect hardware topology
# Each virtual node will share the same physical hardware
NodeName=node6 NodeHostname=HOSTNAME Port=17016 \
    CPUs=112 RealMemory=240000 Gres=gpu:1 State=UNKNOWN

NodeName=node7 NodeHostname=HOSTNAME Port=17017 \
    CPUs=112 RealMemory=240000 Gres=gpu:1 State=UNKNOWN

# GPU partition with 2 nodes
PartitionName=gpu Nodes=node6,node7 Default=YES MaxTime=INFINITE State=UP OverSubscribe=NO
