简短结论先给出：

这是 **generic 的 Tensor Parallel（TP）教学/演示代码**，**不是 vLLM 或 SGLang 的 production 实现**；但**设计范式、API 形态和切分策略是明确“对齐 vLLM / Megatron-LM 的”**，而不是 SGLang 特有的。

下面我分层说清楚。

一、它“是不是 vLLM / SGLang 的代码？”

不是。

这套代码：

* 不依赖 vLLM
* 不依赖 SGLang
* 不使用任何 vLLM 的 kernel、PagedAttention、ModelRunner、Execution Engine
* 不涉及 SGLang 的 IR、RadixAttention、request-level routing

它是一个**纯 PyTorch + torch.distributed 的 TP reference implementation**，用于教学和概念验证 。

二、那它“像不像 vLLM / Megatron 的 TP？”

非常像，而且是**有意为之**。

你这几份文件的结构，基本就是：

* parallel_state.py
  → Megatron / vLLM 风格的 tensor parallel group 管理
  → get_tensor_model_parallel_rank / world_size / all_reduce / all_gather


* linear.py
  → ColumnParallelLinear
  → RowParallelLinear
  → 这是 Megatron-LM / vLLM 的“正宗切法”，不是随便写的


* mlp.py
  → ColumnParallel（up / gate）+ activation + RowParallel（down）
  → 完全等价于 LLaMA / Qwen / GPT 系列在 TP 下的 MLP 结构


如果你熟悉 vLLM 的：

* linear.py
* parallel_state.py
* megatron/core/tensor_parallel/

你会发现这里的“数学结构”和“通信语义”是同一套。

可以明确说一句：

这套代码在 **“TP 数学与通信层面”≈ vLLM / Megatron-LM**。

三、那它和 SGLang 的关系是什么？

非常重要的一点：

SGLang **并不是一个 TP 框架**。

SGLang：

* 不重新发明 TP
* 底层 **直接复用 vLLM 的 TP / attention / kernel**
* 核心创新在：

  * request-level routing
  * program IR
  * structured decoding
  * multi-request scheduling

所以：

* 你这个 TP demo **不“像 SGLang”**
* 但它 **完全可以作为 SGLang 底层所假设的 TP 行为模型**

换句话说：

SGLang =
“假设你已经有一个 vLLM / Megatron-style TP executor”

你现在这份代码 =
“那个 executor 的最小可解释版本”

四、准确分类一句话

如果要给它一个“定位标签”，最准确的是：

这是一个
“Megatron / vLLM-style Tensor Parallelism — minimal, generic, educational implementation”。

不是：

* vLLM production code
* SGLang execution engine
* kernel-level TP
* inference-optimized runtime

但非常适合：

* 书里讲 TP
* 课堂 demo
* 对照解释 vLLM / Megatron 的 linear / MLP
* 给读者一个“能跑、能看 shape、能看到 all-reduce 的 TP”

五、一句给你写书用的评价（你可以直接用）

如果你愿意，我可以帮你润色成书面版：

“Although this implementation is framework-agnostic and written purely in PyTorch, its tensor-parallel abstractions—ColumnParallelLinear, RowParallelLinear, and TP group management—closely follow the design established by Megatron-LM and adopted by vLLM. SGLang, in turn, builds on top of such executors rather than redefining tensor parallelism itself.”

要把你现在这套“教学版 TP”升级到“更接近 vLLM 真正的 TP（用于推理/Serving）”，核心不是再写几个 all-reduce / all-gather，而是把 TP 嵌进一整套推理执行栈：权重装载与分片、KV cache 与注意力、调度与批处理、以及大量“为推理而生”的工程细节。下面按模块给你一份最短路径的改造清单（从必须到加分项）。

1. 把 TP 从“demo layer”升级为“真实模型权重分片装载”
   你现在的 Column/RowParallelLinear 是运行时随机初始化权重。vLLM 的关键是：加载 HuggingFace 权重时就按 TP rank 分片，只把本 rank 的 shard 放进显存。
   你需要做：

* 设计 weight_loader：给定 full weight 的 tensor（或从 safetensors 读出来的 tensor），按 tp_rank 切片（列切/行切）后拷贝到 Parameter。
* 支持常见命名与结构：QKV（可能 fused）、gate/up/down（MLP）、o_proj、embed、lm_head。
* 处理 tied weights（embedding 和 lm_head 共享）时的分片规则。
* 明确每个权重的“切分维度”：例如 ColumnParallelLinear 通常切 out_features；RowParallelLinear 通常切 in_features。

2. 让 TP 贯穿 Transformer Block（不仅仅是 MLP）
   vLLM 的 TP 最关键发生在 attention + MLP 两大块。
   你需要补齐：

* Attention 的 QKV 投影：通常是 ColumnParallelLinear（把 head 维度相关的输出切分到各 rank）
* Attention 的输出投影 o_proj：通常是 RowParallelLinear（汇总各 rank 的 partial）
* MLP 你已经有了，但要对齐真实模型的激活、门控形式（SwiGLU/SiLU 等）与权重布局
* LayerNorm / RMSNorm：通常不切分，但要对齐推理时的实现（最好用 fused/高效实现，后面讲）

3. 引入真正的推理态 KV Cache（这是 vLLM 之所以是 vLLM 的核心之一）
   你的 demo 没有 KV cache；而 vLLM 的 TP 推理必须在“分块 KV cache + 注意力 kernel”上工作。
   你需要做：

* KV cache 的张量形状与布局：按 tp_rank 存自己那部分 heads（TP 会把 heads 分到各 rank），所以每张卡只存自己负责的 heads 的 K/V。
* Block/page 管理：把每个序列的 KV 按 block 分配与映射（block_table、slot_mapping），支持不同长度请求的动态拼 batch。
* Prefill 与 Decode 两条路径：prefill 写入一段连续 tokens；decode 每步追加 1 token（或小 batch 的 tokens）。

如果你不想写 kernel，你仍然可以先用纯 PyTorch 做一个“正确但慢”的版本来把语义跑通：

* Prefill：用 torch.matmul/torch.einsum 做 attention，并把 K/V append 到 cache
* Decode：只算新 token 对历史 KV 的 attention，并写入 cache
  等语义正确后，再决定是否替换为更接近 vLLM 的 PagedAttention kernel（如果你坚持不写 kernel，那就只能停留在“结构接近”而非“性能接近”）。

4. 对齐 vLLM 的“连续批处理（continuous batching）+ 调度元数据”
   vLLM 的 TP 不是“单个 batch 一次 forward”，而是“每 step 动态把来自不同 request 的 token 拼成一个执行单元”，并维护大量 metadata。
   你需要做：

* 序列状态管理：每个 request 的已生成长度、当前位置、已分配 blocks
* 每 step 构建 metadata：position ids、slot_mapping、block_table（或等价物）
* 支持多序列、不同长度同 step 混跑（这会直接影响 KV cache 的索引方式）

这一步做完，你的系统才会开始像“真正的 vLLM 执行器”，而不是“分布式线性层 demo”。

5. 把通信语义升级到更贴近 vLLM 的实际做法
   你现在的 all_reduce / all_gather 是最朴素版本。vLLM 里更关注：

* 通信与计算的重叠（overlap）：例如某些场景可以把通信放到 stream 上并发
* 更少的 gather：尽量避免中间层 all-gather（你在 MLP 已经体现了这个思想）
* 更贴近模型结构的通信点：attention 输出、MLP 输出、最终 logits（是否需要 all-gather 取决于 vocab/head 的切分策略）

6. 推理专用优化（加分项，但在“像 vLLM”上很关键）
   这些不是“TP 正确性”必须，但它们决定你是不是在做“vLLM 那类系统”：

* 禁用 autograd、用 inference_mode、用更严格的 tensor layout（contiguous、正确的 dtype）
* CUDA Graphs（或类似机制）：减少 launch overhead（尤其 decode 每步很短）
* Prefill/Decode 分离执行路径：不同的 kernel/路径（哪怕你不写 kernel，也要分开）
* 采样与 logits 处理：top-k/top-p、temperature、repetition penalty 等在 TP 下的处理（有些需要跨 rank 合并或一致性逻辑）
* 量化/FP8/INT8（若你关心 vLLM 的主流用法）

7. 对齐模型细节（以 Qwen2.5 为例你会遇到的坑）
   你提过用 Qwen/Qwen2.5-0.5B-Instruct 做 demo；要像 vLLM，你还需要：

* RoPE：位置编码的实现与缓存策略（prefill 与 decode 位置不同）
* GQA/MQA：heads 的切分规则会变；TP rank 如何分配 KV heads vs Q heads
* QKV 是否 fused：很多模型把 QKV 合在一个大矩阵，分片要按正确维度切，不然数值会错
* RMSNorm vs LayerNorm：不同模型用不同 norm，权重名也不同

一条建议的最短实施路线（你能最快看到“像 vLLM”的效果）
第一阶段（1–2 天代码量）：把权重加载分片 + 把 TP 套进一个最小 Transformer block（含 attention/MLP，但 attention 先用 torch 实现，先不做 paged KV）
第二阶段：做 KV cache（先连续 append 的朴素版本），跑通 prefill+decode 多步生成
第三阶段：把 KV cache 换成 block/page 管理，做 continuous batching 的 metadata（这一步最像 vLLM）
第四阶段：再考虑性能优化（kernel、CUDA graph、fused op）

如果你把你现在的目录结构和你准备对齐的目标模型（比如 Qwen2.5 的具体 attention/MLP 形式：GQA 还是 MHA、QKV 是否 fused）告诉我，我可以直接给你一套“按 vLLM 风格拆文件”的最小 patch 计划：哪些新建文件、每个文件放哪些类、以及每一步的可运行测试（prefill correctness、decode correctness、跨 rank 一致性、KV 写入/读取一致性）。

结论先给出（非常明确）：

是的，你这个“丐版 PA”**必须用，而且是你走向“真实 vLLM-style TP”的关键资产**；
但**必须改，而且不是小改，是“定位升级式改造”**。

现在它的状态是：
“**语义方向正确，但还没进入 vLLM 的 TP+PA 真实约束空间**”。

下面我按「是否要用 → 现在离真实差在哪 → 为了 TP 需要怎么改 → 哪些可以不改」来系统回答。

一、要不要用？
结论：必须用，而且比你现在的 TP 代码更“vLLM”。

原因很简单：

* 真正的 vLLM TP ≠ Linear/MLP 的 TP
* 真正的 vLLM TP = **Attention + KV cache 的 TP**
* 而 Attention 的 TP 100% 依赖 PagedAttention 的 KV 组织方式

你现在已经有：

* BlockManager（block/page 级 KV 管理）
* slot_mapping 的概念
* ragged batching 的 metadata（seq_id / position / slot）
* prefill / decode 的调度器

这几样东西，**已经超过 90% 的“普通 TP demo”**，反而是 vLLM 的核心思想。

所以：
不用它，你的 TP 永远是“训练 demo 级”；
用它，你才可能进入“推理系统级 TP”。

二、现在这套 PA 能不能“直接”接上 TP？
结论：不能直接，需要“结构性对齐 TP”。

你现在的 PA 是“单卡视角”的：

* BlockManager 假设：

  * num_heads = 全部 heads
  * k_cache / v_cache = 全量 heads
* PagedAttentionV4 的 compute_attention：

  * 默认所有 heads 都在本 rank
* Scheduler：

  * 完全 unaware of TP rank / TP group

而在真实 vLLM TP 下，**以下三件事是硬约束**：

1. heads 是被 TP 切分的
2. KV cache 是“每个 TP rank 只存自己那部分 heads”
3. Attention 的输出在某些点必须 all-reduce / 不 all-gather（非常关键）

所以：
现在这套 PA **在“数学上是对的”**，
但**在“并行语义上还是单卡模型”**。

三、为了“更接近真实 LLM 的 TP”，PA 需要改什么（核心清单）

下面是重点，请你对照着看。

改造目标一句话版：
“让你的 PA 从『单卡 PA』升级为『TP-aware PA』”。

（一）BlockManager：必须变成 TP-aware（这是第一刀）

现在：

* Block.k_cache: [block_size, num_heads, head_dim]

真实 vLLM TP 下应该是：

* 每个 TP rank 只存自己负责的 KV heads
* 即：

  * num_kv_heads_local = num_kv_heads / tp_size
  * k_cache: [block_size, num_kv_heads_local, head_dim]

你需要改：

* BlockManager 初始化时引入：

  * tp_rank
  * tp_size
  * num_kv_heads_total
* 在每个 rank 上：

  * 只分配 local KV cache
* slot_mapping 仍然是 global token slot（这一点你已经做对了）

这是“PA + TP”的第一性前提。

（二）PagedAttention：attention 计算必须是“local heads + global语义”

现在你做的是（简化）：

* 对每个 token
* 用 q 对全量 K/V 做 attention

真实 TP 下：

* 每个 rank：

  * 只算自己那部分 heads 的 attention
* 然后：

  * 对 o_proj 之前的结果做 RowParallelLinear + all-reduce
  * attention 本身不需要 all-gather heads

所以你需要：

* 明确区分：

  * q_heads_local
  * kv_heads_local
* compute_attention(seq_id, q_local):

  * 只访问本 rank 的 KV blocks
  * 输出 local attention output
* attention 本身不做跨 rank 通信

这点你现在的 PA 逻辑上是“可以拆”的，但代码层面还没显式表达。

（三）Ragged metadata：slot_mapping 要 TP-safe（你这点是“半对”）

你现在的 slot_mapping：

* slot = block_id * block_size + offset

在 TP 下：

* block_id 是“全局 block id”
* 但每个 rank：

  * block_id → local physical block index
* 或者：

  * block_id 在所有 ranks 对齐，但 block 内只存 local heads

vLLM 选的是第二种（逻辑 block 对齐，物理 KV 分 head shard）。

你的 BlockManager 已经很接近这种设计了，只需要：

* 保证 block_id 是全局一致
* 每个 rank 的 Block 只含 local heads

这是好消息：你不用推翻 slot_mapping 设计。

（四）Scheduler：必须知道 TP 的存在（但改动最小）

Scheduler 本身不用“理解 TP 数学”，但必须：

* 确保：

  * 同一个 step 的 seq_ids / positions
  * 在所有 TP ranks 完全一致
* 即：

  * 调度是 data-parallel across TP ranks
  * 不是每个 rank 自己调度

在 vLLM 中：

* scheduler 是“全局逻辑”
* TP ranks 是“执行副本”

你现在的 scheduler 是单进程逻辑；
如果你只是做 TP demo：

* 可以先假设 scheduler 在 rank0 跑
* 然后 broadcast metadata 到其他 ranks

不需要复杂化。

四、哪些地方“现在不用改”，别过早优化

下面这些你现在可以不动：

* online softmax（对 TP 正确性不是关键）
* 真正的 single-kernel ragged attention（你已经在注释里说“这是示意”）
* CUDA graph / kernel fusion
* 真正的 vLLM PagedAttention kernel

为了“更接近真实 LLM 的 TP”，
**语义 > 并行关系 > 内存布局 > 性能**
性能是最后一步。

五、一句非常重要的判断（给你一个方向锚点）

如果你现在做的是：

* TP Linear / MLP + 单卡 attention
  → 你在做“训练并行 demo”

如果你把：

* 这个 PA 改成 TP-aware
  → 你在做“推理系统 executor”

这是一个**质变**，不是量变。

六、我给你的下一步建议（非常具体）

下一步不要再写新模块了，直接做这三件事：

1. 给 BlockManager 加 tp_rank / tp_size / local_kv_heads
2. 改 PagedAttention，使 attention 只用 local KV
3. 用你现有 scheduler + PA + TP MLP
   跑一个：

   * 2 TP ranks
   * 2 sequences
   * 不同 prompt length
   * prefill + decode 3 步
   * 验证输出在 TP rank 间一致

如果你愿意，下一轮我可以直接帮你：

* 给 BlockManager / PA 写一版「TP-aware 最小 patch」
* 或者画一张“你当前架构 → vLLM 架构”的精确对照图（用你现在的符号体系）

你这套已经不是“玩具了”，现在是在**往真正系统靠**。
