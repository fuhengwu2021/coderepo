This is a major step forward. With the new files, your TP code is now “vLLM-style” in the two most important senses: (1) real checkpoint weight loading with sharding at load time, and (2) TP-aware attention (QKV sharded, attention local, o_proj row-parallel with all-reduce). Concretely:

1. You are no longer “generic TP only”

* You added a real weight loading path and explicitly frame it as “weights sharded during loading” in demo_vllm.py. 
* You implemented vLLM-like weight_loader patterns inside ColumnParallelLinear / RowParallelLinear / QKVParallelLinear. 

2. You now have a vLLM-style TP Attention layer (correct high-level structure)
   TensorParallelAttention follows the standard design:

* QKV projection is sharded (column parallel over heads)
* attention computation is local (no comm)
* o_proj reduces via row-parallel all-reduce 

So: “how about now?” — It is no longer just a generic TP demo; it is a credible “vLLM-style TP skeleton with real weight loading.”

That said, there are a few correctness and “closeness to real vLLM” issues you should fix next.

A. Critical correctness issue: KV cache return is wrong for incremental decode
In TensorParallelAttention.forward you concatenate cache into k/v, then return (k, v). But after repeat_interleave for GQA you mutate k/v to have num_heads_local heads, and you return that repeated version as cache. That will explode KV size across steps and is semantically wrong (cache should store KV heads, not repeated heads). The problematic flow is:

* concat cache: k/v are [batch, num_kv_heads_local, T, head_dim]
* if GQA: you repeat_interleave to match Q heads
* you return (k, v) (now likely [batch, num_heads_local, T, head_dim]) 

Fix: keep two versions:

* k_kv, v_kv: for storage (num_kv_heads_local)
* k_for_attn, v_for_attn: possibly repeated to num_heads_local
  Return cache as (k_kv, v_kv), not the repeated version.

B. Masking bug: using “ones * -inf” is numerically unsafe
You build:
causal_mask = triu(ones, diag=1) * -inf
This does 0 * -inf on the lower triangle, which yields NaNs in PyTorch. 

Fix: create a boolean mask and masked_fill:
mask = torch.triu(torch.ones(seq_len, seq_len, device=..., dtype=torch.bool), diagonal=1)
scores = scores.masked_fill(mask[None,None,:,:], float("-inf"))

C. QKVParallelLinear sizing logic is very likely wrong / confusing
In QKVParallelLinear.**init**, you compute output_size as:
(num_heads_local + 2*num_kv_heads_local) * tp_size * head_size
and pass that to ColumnParallelLinear which then divides output_size by tp_size again internally. 

This works only if you intentionally “pre-multiply” by tp_size so that after divide you land on the desired local size; but it’s easy to get wrong and will confuse future readers. In vLLM/Megatron, you typically set “global output size” and let ColumnParallelLinear derive “local output size”. Here, you are mixing global/local in a way that is error-prone.

At minimum, add an explicit assertion after init that:
self.weight.shape[0] == (num_heads_local + 2*num_kv_heads_local) * head_size
and that split_qkv boundaries match the produced tensor.

D. Your “state_dict sharding” is not actually sharding the checkpoint yet
TPModelWrapper claims it shards during loading, but load_and_shard_state_dict currently loads the full state dict on every rank (and does not shard tensors there).  

You do shard when calling layer.weight_loader* (good), but memory-wise you still hold full tensors in wrapper.state_dict on each rank. That’s not vLLM production behavior; vLLM avoids loading full weights on every rank.

If your goal is “closer to real vLLM”, next step is:

* rank0 loads full tensors
* broadcast only needed shards (or have each rank load only its shard from safetensors by slicing on load)

E. You still haven’t integrated PagedAttention / block KV into this TP Attention
Right now kv_cache is a simple concatenated tensor pair. That is fine for “minimal incremental decoding,” but it’s not “vLLM-like” because:

* no block_table / slot_mapping
* no paged KV organization
* no continuous batching metadata path

If the target is “more like real vLLM TP,” the next integration step is:

* replace kv_cache=(k,v) with a block-based KV store (your BlockManager + slot_mapping)
* attention reads KV via slot_mapping rather than concatenating tensors

F. Minor: inference_mode inside layers can be awkward
You wrap forward in inference_mode inside linear layers. 
It works, but it’s more typical to put inference_mode at the top-level (demo loop) so you don’t accidentally nest or surprise users. Not a correctness bug—just ergonomics.

Bottom line

* “Now” you have: real weight loading hooks + a TP-aware attention layer + a wrapper that constructs TP layers from HF checkpoints. That is definitively beyond generic TP.  
* To become “closer to real vLLM TP” you should next:

  1. fix KV cache return + mask NaN issue (must-fix correctness)
  2. simplify/validate QKVParallelLinear output sizing (must-fix clarity/correctness)
  3. change weight loading to avoid full state_dict per-rank (closeness-to-vLLM)
  4. integrate your PagedAttention KV (block_table/slot_mapping) into TP attention (the big realism leap)

If you want, I can produce a very small patch plan (exact code edits) for A + B first, because those two will bite you immediately when you try real decode.
