This is a major step forward. With the new files, your TP code is now “vLLM-style” in the two most important senses: (1) real checkpoint weight loading with sharding at load time, and (2) TP-aware attention (QKV sharded, attention local, o_proj row-parallel with all-reduce). Concretely:

1. You are no longer “generic TP only”

* You added a real weight loading path and explicitly frame it as “weights sharded during loading” in demo_vllm.py. 
* You implemented vLLM-like weight_loader patterns inside ColumnParallelLinear / RowParallelLinear / QKVParallelLinear. 

2. You now have a vLLM-style TP Attention layer (correct high-level structure)
   TensorParallelAttention follows the standard design:

* QKV projection is sharded (column parallel over heads)
* attention computation is local (no comm)
* o_proj reduces via row-parallel all-reduce 

So: “how about now?” — It is no longer just a generic TP demo; it is a credible “vLLM-style TP skeleton with real weight loading.”

That said, there are a few correctness and “closeness to real vLLM” issues you should fix next.

A. Critical correctness issue: KV cache return is wrong for incremental decode
In TensorParallelAttention.forward you concatenate cache into k/v, then return (k, v). But after repeat_interleave for GQA you mutate k/v to have num_heads_local heads, and you return that repeated version as cache. That will explode KV size across steps and is semantically wrong (cache should store KV heads, not repeated heads). The problematic flow is:

* concat cache: k/v are [batch, num_kv_heads_local, T, head_dim]
* if GQA: you repeat_interleave to match Q heads
* you return (k, v) (now likely [batch, num_heads_local, T, head_dim]) 

Fix: keep two versions:

* k_kv, v_kv: for storage (num_kv_heads_local)
* k_for_attn, v_for_attn: possibly repeated to num_heads_local
  Return cache as (k_kv, v_kv), not the repeated version.

B. Masking bug: using “ones * -inf” is numerically unsafe
You build:
causal_mask = triu(ones, diag=1) * -inf
This does 0 * -inf on the lower triangle, which yields NaNs in PyTorch. 

Fix: create a boolean mask and masked_fill:
mask = torch.triu(torch.ones(seq_len, seq_len, device=..., dtype=torch.bool), diagonal=1)
scores = scores.masked_fill(mask[None,None,:,:], float("-inf"))

C. QKVParallelLinear sizing logic is very likely wrong / confusing
In QKVParallelLinear.**init**, you compute output_size as:
(num_heads_local + 2*num_kv_heads_local) * tp_size * head_size
and pass that to ColumnParallelLinear which then divides output_size by tp_size again internally. 

This works only if you intentionally “pre-multiply” by tp_size so that after divide you land on the desired local size; but it’s easy to get wrong and will confuse future readers. In vLLM/Megatron, you typically set “global output size” and let ColumnParallelLinear derive “local output size”. Here, you are mixing global/local in a way that is error-prone.

At minimum, add an explicit assertion after init that:
self.weight.shape[0] == (num_heads_local + 2*num_kv_heads_local) * head_size
and that split_qkv boundaries match the produced tensor.

D. Your “state_dict sharding” is not actually sharding the checkpoint yet
TPModelWrapper claims it shards during loading, but load_and_shard_state_dict currently loads the full state dict on every rank (and does not shard tensors there).  

You do shard when calling layer.weight_loader* (good), but memory-wise you still hold full tensors in wrapper.state_dict on each rank. That’s not vLLM production behavior; vLLM avoids loading full weights on every rank.

If your goal is “closer to real vLLM”, next step is:

* rank0 loads full tensors
* broadcast only needed shards (or have each rank load only its shard from safetensors by slicing on load)

E. You still haven’t integrated PagedAttention / block KV into this TP Attention
Right now kv_cache is a simple concatenated tensor pair. That is fine for “minimal incremental decoding,” but it’s not “vLLM-like” because:

* no block_table / slot_mapping
* no paged KV organization
* no continuous batching metadata path

If the target is “more like real vLLM TP,” the next integration step is:

* replace kv_cache=(k,v) with a block-based KV store (your BlockManager + slot_mapping)
* attention reads KV via slot_mapping rather than concatenating tensors

F. Minor: inference_mode inside layers can be awkward
You wrap forward in inference_mode inside linear layers. 
It works, but it’s more typical to put inference_mode at the top-level (demo loop) so you don’t accidentally nest or surprise users. Not a correctness bug—just ergonomics.

Bottom line

* “Now” you have: real weight loading hooks + a TP-aware attention layer + a wrapper that constructs TP layers from HF checkpoints. That is definitively beyond generic TP.  
* To become “closer to real vLLM TP” you should next:

  1. fix KV cache return + mask NaN issue (must-fix correctness)
  2. simplify/validate QKVParallelLinear output sizing (must-fix clarity/correctness)
  3. change weight loading to avoid full state_dict per-rank (closeness-to-vLLM)
  4. integrate your PagedAttention KV (block_table/slot_mapping) into TP attention (the big realism leap)

If you want, I can produce a very small patch plan (exact code edits) for A + B first, because those two will bite you immediately when you try real decode.

Yes—I retain that assessment, with one refinement: the sizing logic is “likely confusing / fragile,” not “necessarily wrong,” and your successful forward pass does not disprove the concern.

Why the demo can succeed even if the sizing logic is questionable

Your run demonstrates that:

* the layer shapes are self-consistent end-to-end for hidden_size=896, tp=2, seq_len=10, and
* the Q/K/V splits match the q_size / kv_size derived in the module.

That’s enough to run a forward pass and print shapes (as your demo does) .

But the specific concern I raised is about how QKVParallelLinear mixes “global vs local” sizing, and how easy it is to get an apparently-working shape that is still semantically misaligned with checkpoint formats or other configs.

What is confusing / fragile in QKVParallelLinear sizing

In QKVParallelLinear.**init**, you compute:

output_size = ((num_heads_local + 2 * num_kv_heads_local) * tp_size * head_size)

then pass that to ColumnParallelLinear, which internally divides output_size by tp_size to produce the local out_features per rank .

So you are effectively “pre-multiplying by tp_size” to cancel out the division inside ColumnParallelLinear.

This can be correct, but it is unconventional and easy to break because:

* ColumnParallelLinear’s output_size parameter is conceptually the global out_features.
* In QKVParallelLinear you are constructing output_size from local counts, then inflating it back to a global number by multiplying tp_size.
* If someone later changes ColumnParallelLinear’s partitioning rule (or reuses QKVParallelLinear with a different ColumnParallelLinear), QKVParallelLinear silently breaks.

Why it still worked here
Because for this specific configuration, the algebra cancels perfectly:

Let:

* H = total_num_heads
* H_kv = total_num_kv_heads
* tp = tp_size
* d = head_size
* num_heads_local = H / tp
* num_kv_heads_local = H_kv / tp (in the non-replication case)

Then:

* your output_size becomes ((H/tp + 2*H_kv/tp) * tp * d) = (H + 2 H_kv) d
* ColumnParallelLinear then divides by tp → local out_features = (H + 2H_kv) d / tp
* which matches (H/tp + 2H_kv/tp) d, i.e., q_size + 2*kv_size per rank.

So: shapes can be consistent and forward passes succeed, even though the design is still confusing.

Two concrete issues you should fix next (independent of sizing)

1. Causal mask produces NaNs
   You do:
   triu(ones, diag=1) * -inf
   This yields 0 * -inf on the lower triangle → NaNs. 
   Use masked_fill with a boolean mask instead.

2. KV cache returned is wrong under GQA
   You repeat_interleave k/v to match Q heads, then return (k, v) as cache. That will store repeated heads and grow memory incorrectly across decode steps. 
   Return the non-repeated KV tensor as cache; only repeat for attention computation.

How to settle the “sizing logic” question definitively

Right now your demo does not test numeric correctness against a non-TP reference. It only tests shape and that the code runs.

To confirm the QKVParallelLinear sizing is truly correct (not just shape-consistent), add one verification:

* Load the same layer weights into a non-TP attention module (plain PyTorch linear layers).
* Run the same input through:
  (a) non-TP attention, and
  (b) TP attention (2 ranks), then gather/compare outputs on rank 0.
* They should match within tolerance.

If they match, your sizing is correct for that model/config. If not, the mismatch usually comes from:

* wrong concatenation order (Q|K|V) assumptions,
* wrong shard dimension (dim=0 vs dim=1 confusion for HF weight layout),
* head replication logic for KV.

Net: I still recommend changing QKVParallelLinear to accept “global output_size” cleanly (no pre-multiply hack), and adding asserts that the produced local weight shape equals (q_size + 2*kv_size, hidden_size). That will make it both correct and maintainable.
