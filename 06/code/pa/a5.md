下面是按你现有“三文件结构”（block_manager.py / paged_attention.py / inference.py）给出的“最小改动 patch 方案”。目标是：在不重写整个框架的前提下，让 PA 不只是“KV 分配器”，而是真正参与 decode 阶段的 attention 计算，同时把你当前代码里几个会直接导致结果不对/跑不起来的问题顺手修掉。

一、你当前代码的关键问题（为什么必须改）

1. inference.py 里 from_pretrained 用了 dtype=…（HF 里是 torch_dtype=…），以及 device_map=device（传字符串不一定合法）。这会直接跑不起来或行为不稳定。
2. decode_step 当前做法：拿“最后一个 prompt token”当输入 token，然后又把新 token 的 KV append 进 cache。这样会重复计算/重复写入 KV，位置也会错，生成质量会非常怪。
3. 你的 BlockManager.append_kv 接收 token_idx，但完全没用它；一旦上层出现“非严格顺序 append”，会静默写错，不容易排查。
4. 如果你要自己实现“最简 PA + Qwen2Attention/RoPE/GQA”，decode 阶段必须能从 Qwen2Attention 里拿到 rotary_emb 并正确对 Q/K 做 RoPE。

二、Patch 的设计点（尽量不动结构）

1. Prefill 仍然走 HF forward(use_cache=True)，把 HF 产出的 KV（已经 RoPE’d）写入 PA blocks。
2. Decode 改成“手写一 token 的 per-layer forward”，并在每层调用 PagedAttention.compute_attention(seq_id, q_tok)。
3. 为了避免“prefill 已经算出 first token 但你没把它纳入 decode 的 cache”，用一个 one-token lookahead：prefill 先把 first_token_id 存到 seq_info[“next_token_id”]，decode_step 每次“发射 token_to_emit”，同时算出下一 token 并更新 next_token_id。
4. RoPE：优先用 layer.self_attn.rotary_emb（Qwen2Attention 内部的），兼容不同 transformers 版本的调用签名。
5. GQA：KV heads 少于 Q heads 时，repeat_interleave 到 num_heads（最简做法，虽然不高效，但改动最小）。

三、直接可用的 unified diff（只改 inference.py + block_manager.py；paged_attention.py 不需要改）

1. inference.py（核心：替换 decode_step + 修 from_pretrained + 加 RoPE + prefill 存 next_token_id）
   把下面 diff 应用到你的 inference.py。

```diff
--- inference.py
+++ inference.py
@@
-"""
+""" 
 Inference script using custom PagedAttention for KV cache management.
 
 This script demonstrates how to use the custom PagedAttention implementation
-to manage KV cache when doing inference with Qwen/Qwen2.5-0.5B-Instruct.
+for KV cache management and (minimally) integrate it into the attention compute
+path during decode for Qwen/Qwen2.5-0.5B-Instruct.
+
+Design (intentionally minimal):
+- Prefill: use HuggingFace forward(use_cache=True) and write the returned KV
+  (already RoPE'd) into PagedAttention blocks.
+- Decode: run a minimal manual per-layer forward for ONE token, calling
+  PagedAttention.compute_attention(...) so PA participates in the compute path.
 """
 
+import os
+import sys
+from typing import Optional, Tuple
+
 import torch
-import torch.nn as nn
-import torch.nn.functional as F
 from transformers import AutoModelForCausalLM, AutoTokenizer
-from typing import List, Optional, Tuple
-import sys
-import os
+from transformers.models.qwen2.modeling_qwen2 import apply_rotary_pos_emb
 
 # Add parent directory to path to import pa module
 sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
 
 from pa import PagedAttention
 
@@
 class PagedAttentionModelWrapper:
@@
     def __init__(
         self,
         model_name: str = "Qwen/Qwen2.5-0.5B-Instruct",
         block_size: int = 16,
-        device: str = "cuda"
+        device: str = "cuda",
     ):
         self.device = device
         self.block_size = block_size
@@
-        self.model = AutoModelForCausalLM.from_pretrained(
-            model_name,
-            dtype=torch.float16,
-            device_map=device
-        )
+        torch_dtype = torch.float16 if device.startswith("cuda") else torch.float32
+        self.model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch_dtype)
+        self.model.to(device)
+        self.model.eval()
@@
-        self.num_heads = config.num_attention_heads
-        self.num_kv_heads = getattr(config, 'num_key_value_heads', self.num_heads)
-        self.head_dim = config.hidden_size // config.num_attention_heads
-        self.num_layers = config.num_hidden_layers
+        self.num_heads = int(config.num_attention_heads)
+        self.num_kv_heads = int(getattr(config, "num_key_value_heads", self.num_heads))
+        self.head_dim = int(config.hidden_size // config.num_attention_heads)
+        self.num_layers = int(config.num_hidden_layers)
@@
-        self.sequences: dict[int, dict] = {}  # seq_id -> {prompt_tokens, generated_tokens, ...}
+        self.sequences: dict[int, dict] = {}
         self.next_seq_id = 0
+
+    def _apply_rope(
+        self,
+        attn_module,
+        query_states: torch.Tensor,
+        key_states: torch.Tensor,
+        position_ids: torch.Tensor,
+        kv_seq_len: int,
+    ) -> Tuple[torch.Tensor, torch.Tensor]:
+        rotary_emb = getattr(attn_module, "rotary_emb", None)
+        if rotary_emb is None:
+            rotary_emb = getattr(self.model.model, "rotary_emb", None)
+        if rotary_emb is None:
+            raise RuntimeError("Could not find rotary_emb on attention module or model.")
+
+        cos_sin = None
+        for call in (
+            lambda: rotary_emb(key_states, position_ids),
+            lambda: rotary_emb(key_states, seq_len=kv_seq_len),
+            lambda: rotary_emb(key_states, kv_seq_len),
+        ):
+            try:
+                cos_sin = call()
+                break
+            except TypeError:
+                continue
+        if cos_sin is None:
+            raise RuntimeError("Failed to call rotary_emb with supported signatures.")
+
+        cos, sin = cos_sin
+        try:
+            q, k = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)
+        except TypeError:
+            q, k = apply_rotary_pos_emb(query_states, key_states, cos, sin)
+        return q, k
 
@@
     def prefill(self, prompt: str, seq_id: Optional[int] = None) -> int:
@@
         self.sequences[seq_id] = {
             "prompt_tokens": prompt_tokens,
             "generated_tokens": [],
-            "total_tokens": len(prompt_tokens),  # Track total tokens for this sequence
+            "total_tokens": len(prompt_tokens),  # number of tokens already cached
+            "next_token_id": None,               # one-token lookahead buffer
         }
@@
-        # Check what the first token would be
-        logits_check = outputs.logits[:, -1, :]
-        first_token_id = torch.argmax(logits_check, dim=-1).item()
+        logits_check = outputs.logits[:, -1, :]
+        first_token_id = int(torch.argmax(logits_check, dim=-1).item())
+        self.sequences[seq_id]["next_token_id"] = first_token_id
 
@@
-    def decode_step(self, seq_id: int) -> Optional[int]:
-        """
-        Generate one token using PagedAttention for KV cache lookup.
-        ...
-        """
-        if seq_id not in self.sequences:
-            return None
-        ...
-        return next_token_id
+    def decode_step(self, seq_id: int) -> Optional[int]:
+        \"\"\"Generate one token, using PagedAttention for attention computation.\"\"\"
+        if seq_id not in self.sequences:
+            return None
+
+        seq_info = self.sequences[seq_id]
+
+        # one-token lookahead:
+        # - emit seq_info['next_token_id'] now (and cache it)
+        # - compute next token id and store back to seq_info['next_token_id']
+        token_to_emit = seq_info.get(\"next_token_id\")
+        if token_to_emit is None:
+            token_to_emit = seq_info[\"prompt_tokens\"][-1]
+
+        token_tensor = torch.tensor([[int(token_to_emit)]], device=self.device)
+        position = int(seq_info[\"total_tokens\"])
+        position_ids = torch.tensor([[position]], device=self.device, dtype=torch.long)
+        kv_seq_len = position + 1
+
+        with torch.no_grad():
+            hidden_states = self.model.model.embed_tokens(token_tensor)  # [1,1,H]
+
+            for layer_idx in range(self.num_layers):
+                layer = self.model.model.layers[layer_idx]
+                attn = layer.self_attn
+
+                residual = hidden_states
+                hidden_states = layer.input_layernorm(hidden_states)
+
+                q = attn.q_proj(hidden_states)
+                k = attn.k_proj(hidden_states)
+                v = attn.v_proj(hidden_states)
+
+                q = q.view(1, 1, self.num_heads, self.head_dim).transpose(1, 2)      # [1,Hq,1,D]
+                k = k.view(1, 1, self.num_kv_heads, self.head_dim).transpose(1, 2)   # [1,Hkv,1,D]
+                v = v.view(1, 1, self.num_kv_heads, self.head_dim).transpose(1, 2)   # [1,Hkv,1,D]
+
+                q, k = self._apply_rope(attn, q, k, position_ids, kv_seq_len)
+
+                q_tok = q[0, :, 0, :]   # [Hq,D]
+                k_tok = k[0, :, 0, :]   # [Hkv,D]
+                v_tok = v[0, :, 0, :]   # [Hkv,D]
+
+                if self.num_kv_heads < self.num_heads:
+                    repeat_factor = self.num_heads // self.num_kv_heads
+                    k_tok = k_tok.repeat_interleave(repeat_factor, dim=0)  # [Hq,D]
+                    v_tok = v_tok.repeat_interleave(repeat_factor, dim=0)
+
+                # IMPORTANT: append KV first so attention includes self (causal allows self)
+                self.paged_attentions[layer_idx].append_kv(seq_id, k_tok, v_tok, position)
+
+                # Attention via PagedAttention (no padding; iterates only allocated blocks)
+                ctx = self.paged_attentions[layer_idx].compute_attention(seq_id, q_tok)
+
+                ctx = ctx.reshape(1, 1, self.num_heads * self.head_dim)
+                attn_out = attn.o_proj(ctx)
+                hidden_states = residual + attn_out
+
+                residual = hidden_states
+                hidden_states = layer.post_attention_layernorm(hidden_states)
+                mlp_out = layer.mlp(hidden_states)
+                hidden_states = residual + mlp_out
+
+            hidden_states = self.model.model.norm(hidden_states)
+            logits = self.model.lm_head(hidden_states)  # [1,1,vocab]
+            next_token_id = int(torch.argmax(logits[0, -1, :]).item())
+
+        seq_info[\"generated_tokens\"].append(int(token_to_emit))
+        seq_info[\"total_tokens\"] += 1
+        seq_info[\"next_token_id\"] = next_token_id
+
+        return int(token_to_emit)
```

2. block_manager.py（最小：用上 token_idx + CPU dtype 修正）
   把下面 diff 应用到你的 block_manager.py。

```diff
--- block_manager.py
+++ block_manager.py
@@
         k_cache = torch.zeros(
             (self.block_size, self.num_heads, self.head_dim),
-            dtype=torch.float16,
+            dtype=(torch.float16 if str(self.device).startswith('cuda') else torch.float32),
             device=self.device
         )
         v_cache = torch.zeros(
             (self.block_size, self.num_heads, self.head_dim),
-            dtype=torch.float16,
+            dtype=(torch.float16 if str(self.device).startswith('cuda') else torch.float32),
             device=self.device
         )
@@
         block_table = self.sequence_tables[seq_id]
+
+        # Enforce sequential appends for this minimal demo.
+        # In a full PA implementation, token_idx is used to map logical positions
+        # to physical blocks (supporting out-of-order writes).
+        expected_idx = block_table.get_total_tokens()
+        if token_idx != expected_idx:
+            raise ValueError(
+                f\"Non-sequential token_idx for seq_id={seq_id}: got {token_idx}, expected {expected_idx}.\"
+            )
```

3. paged_attention.py
   不需要改。你现在的 compute_attention 是“拼接已有 blocks，然后做 matmul”，已经满足“无 padding token 的 attention 遍历”，只是性能不高（但你要的是最简 PA，从 0 起步完全 OK）。

四、这个 patch 改完后，你的 PA 在 Qwen2.5-0.5B-Instruct 上是怎么工作的

1. prefill(prompt)：HF 一次性跑完整个 prompt，拿到 past_key_values（KV 已经 RoPE’d），逐 token 写入 BlockManager。
2. decode_step：

* 从 seq_info[“next_token_id”] 取出 token_to_emit（prefill 已经算好第一步要生成的 token）
* 用 Qwen2Attention.q_proj/k_proj/v_proj 算出当前 token 的 Q/K/V，并用 Qwen2Attention.rotary_emb + apply_rotary_pos_emb 对 Q/K 做 RoPE
* 把当前 token 的 K/V 先 append 到 blocks（保证 attention 里包含 self）
* 调用 PagedAttention.compute_attention 只遍历已有 blocks（没有 padding）
* o_proj + 残差 + MLP，得到 logits，算出下一 token，写回 next_token_id
* 返回本步 emit 的 token（外层 generate 收集它）

五、你后续如果要把它从“最简 demo”升级（不在本 patch 范围内，但给你一个路线）

1. 真正的“PagedAttention 解决 fragmentation”要做 block table + slot mapping（token_idx -> (block_id, offset)），并支持并发 sequence、block recycle、prefix sharing、swap 等。
2. 真正的高性能 PA 需要 kernel 级别避免 torch.cat（现在 cat 会产生临时大 tensor，虽然没有 padding，但仍然会产生额外带宽/内存开销）。
3. 真正 batch decode：compute_attention_batch 要把不同 seq 的 block table 合并成一个 kernel 调度，而不是 Python for-loop。

如果你愿意，我可以在你这个最简版本上，继续按“最小增量”方式加两件最有价值的东西：batch decode（多请求同时 decode）和不做 torch.cat 的 block-streaming attention（仍然纯 PyTorch，但避免大拼接）。
