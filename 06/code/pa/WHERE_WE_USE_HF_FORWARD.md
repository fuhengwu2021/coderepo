# æˆ‘ä»¬åœ¨å“ªé‡Œä½¿ç”¨äº† HuggingFace çš„ forward()ï¼Ÿ

## è°ƒç”¨ä½ç½®

### 1. Prefill é˜¶æ®µï¼ˆä¸»è¦é™åˆ¶ï¼‰

**ä½ç½®**ï¼š`inference_v4.py` ç¬¬ 191 è¡Œ

```python
def prefill_batch(self) -> int:
    # ... æ„å»º metadata ...
    
    with torch.no_grad():
        # âŒ é—®é¢˜ï¼šå¿…é¡»åˆ†åˆ«å¤„ç†æ¯ä¸ªåºåˆ—
        for i, (seq_id, prompt_tokens) in enumerate(zip(seq_ids, prompt_token_lists)):
            seq_tokens = torch.tensor([prompt_tokens], device=self.device)  # [1, L_i]
            
            # ğŸ”´ è¿™é‡Œè°ƒç”¨äº† HuggingFace çš„ forward()
            outputs = self.model(input_ids=seq_tokens, use_cache=True)
            # ç­‰ä»·äºï¼šoutputs = self.model.forward(input_ids=seq_tokens, use_cache=True)
            
            past_key_values = outputs.past_key_values
            # ... æå–å¹¶å­˜å‚¨ KV cache ...
```

**é—®é¢˜**ï¼š
- æˆ‘ä»¬**æ— æ³•**ä¸€æ¬¡æ€§ä¼ å…¥æ‰€æœ‰ flattened tokens `[1, T]`ï¼ˆT = 109 tokensï¼‰
- å¿…é¡»**åˆ†åˆ«**ä¸ºæ¯ä¸ªåºåˆ—è°ƒç”¨ `model.forward()`
- å¦‚æœæœ‰ 3 ä¸ªåºåˆ—ï¼Œå°±è¦è°ƒç”¨ 3 æ¬¡ `model.forward()`

**å¦‚æœå°è¯•ä¼ å…¥ flattened tokens**ï¼š
```python
# âŒ è¿™æ ·ä¸è¡Œ
token_ids_flat = [109 tokens]  # 3 ä¸ªåºåˆ—æ‘Šå¹³
token_tensor = torch.tensor([token_ids_flat])  # [1, 109]
outputs = self.model(input_ids=token_tensor)  # âŒ é”™è¯¯ï¼
# HuggingFace ä¼šæŠŠæ‰€æœ‰ 109 ä¸ª tokens å½“ä½œä¸€ä¸ªé•¿åºåˆ—
# åºåˆ— 0 çš„ tokens ä¼š attend åˆ°åºåˆ— 1 å’Œåºåˆ— 2 çš„ tokensï¼ˆé”™è¯¯ï¼ï¼‰
```

### 2. Decode é˜¶æ®µï¼ˆéƒ¨åˆ†ä½¿ç”¨ï¼‰

**ä½ç½®**ï¼š`inference_v4.py` ç¬¬ 243 è¡Œ

```python
def decode_batch(self) -> Tuple[List[int], List[int]]:
    # ... è·å– decode åºåˆ— ...
    
    token_tensor = torch.tensor([token_ids], device=self.device)  # [1, num_seqs]
    
    with torch.no_grad():
        # âœ… åªä½¿ç”¨ embedding å±‚ï¼ˆè¿™ä¸ªæ²¡é—®é¢˜ï¼‰
        hidden_states = self.model.model.embed_tokens(token_tensor)  # [1, num_seqs, H]
        
        # âœ… ç„¶åæ‰‹åŠ¨éå†æ¯ä¸€å±‚ï¼ˆä¸ä½¿ç”¨å®Œæ•´çš„ forwardï¼‰
        for i, seq_id in enumerate(seq_ids):
            seq_hidden = hidden_states[:, i:i+1, :]  # [1, 1, H]
            
            # æ‰‹åŠ¨å¤„ç†æ¯ä¸€å±‚
            for layer_idx in range(self.num_layers):
                layer = self.model.model.layers[layer_idx]
                # ... æ‰‹åŠ¨è®¡ç®— Q, K, V, attention ...
                # ä½¿ç”¨ PagedAttention çš„ compute_attention()
                attn_output = self.paged_attentions[layer_idx].compute_attention(seq_id, q_tok)
```

**è¯´æ˜**ï¼š
- Decode é˜¶æ®µ**æ²¡æœ‰**ä½¿ç”¨å®Œæ•´çš„ `model.forward()`
- åªä½¿ç”¨äº† `embed_tokens`ï¼ˆembedding å±‚ï¼‰
- ç„¶åæ‰‹åŠ¨éå†æ¯ä¸€å±‚ï¼Œä½¿ç”¨ PagedAttention è®¡ç®— attention
- æ‰€ä»¥ decode é˜¶æ®µä¸å— HuggingFace é™åˆ¶

## ä¸ºä»€ä¹ˆ Prefill é˜¶æ®µå¿…é¡»ä½¿ç”¨ HuggingFace forwardï¼Ÿ

### åŸå›  1ï¼šéœ€è¦å®Œæ•´çš„æ¨¡å‹è®¡ç®—

Prefill é˜¶æ®µéœ€è¦ï¼š
1. æ‰€æœ‰å±‚çš„ forward pass
2. RoPE ä½ç½®ç¼–ç 
3. Attention è®¡ç®—ï¼ˆåŒ…æ‹¬ causal maskï¼‰
4. MLP è®¡ç®—
5. Layer normalization

å¦‚æœæ‰‹åŠ¨å®ç°ï¼Œä»£ç ä¼šéå¸¸å¤æ‚ã€‚

### åŸå›  2ï¼šRoPE çš„æ­£ç¡®åº”ç”¨

```python
# HuggingFace çš„ forward() ä¼šè‡ªåŠ¨å¤„ç† RoPE
outputs = self.model(input_ids=seq_tokens, use_cache=True)
# å†…éƒ¨ä¼šï¼š
# 1. è®¡ç®— position_ids
# 2. åº”ç”¨ RoPE åˆ° Q, K
# 3. è®¡ç®— attention
# 4. è¿”å› past_key_valuesï¼ˆå·²ç»åº”ç”¨äº† RoPE çš„ K, Vï¼‰
```

### åŸå›  3ï¼šè·å– past_key_values

```python
# HuggingFace çš„ forward() è¿”å›çš„ past_key_values æ ¼å¼ï¼š
past_key_values = [
    (k_layer0, v_layer0),  # [1, num_kv_heads, seq_len, head_dim]
    (k_layer1, v_layer1),
    ...
]

# æˆ‘ä»¬å¯ä»¥ç›´æ¥æå–å¹¶å­˜å‚¨åˆ° PagedAttention blocks
for layer_idx in range(self.num_layers):
    k, v = past_key_values[layer_idx]
    # k, v å·²ç»åº”ç”¨äº† RoPEï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨
```

## å¯¹æ¯”ï¼švLLM æ˜¯å¦‚ä½•åšçš„ï¼Ÿ

### vLLM çš„ Prefill é˜¶æ®µ

```python
# vLLM ä¸ä½¿ç”¨ HuggingFace çš„ forward()
# è€Œæ˜¯ä½¿ç”¨è‡ªå®šä¹‰çš„ CUDA kernels

def prefill_batch(self, input_ids_flat, seq_id_flat, position_flat):
    # 1. è‡ªå®šä¹‰ embedding
    hidden_states = self.embed_tokens(input_ids_flat)  # [T, H]
    
    # 2. è‡ªå®šä¹‰ attention kernelï¼ˆå¤„ç† flattened tokensï¼‰
    for layer in self.layers:
        # ä½¿ç”¨è‡ªå®šä¹‰çš„ paged_attention_kernel
        attn_output = paged_attention_kernel(
            q=hidden_states,           # [T, H]
            seq_id_flat=seq_id_flat,   # [T] - metadata
            position_flat=position_flat, # [T] - metadata
            block_tables=block_tables,  # Dict[int, List[int]]
            ...
        )
        hidden_states = layer.mlp(attn_output)
    
    # 3. æ‰€æœ‰ T ä¸ª tokens åœ¨ä¸€ä¸ª kernel ä¸­å¤„ç†
    # ä¸éœ€è¦åˆ†åˆ«å¤„ç†æ¯ä¸ªåºåˆ—
```

**å…³é”®åŒºåˆ«**ï¼š
- vLLMï¼šè‡ªå®šä¹‰ CUDA kernelsï¼Œå¯ä»¥å¤„ç† `[T]` + metadata
- æˆ‘ä»¬ï¼šä½¿ç”¨ HuggingFace forwardï¼Œåªèƒ½å¤„ç† `[1, L_i]`ï¼ˆæ¯ä¸ªåºåˆ—å•ç‹¬å¤„ç†ï¼‰

## æ€»ç»“

### æˆ‘ä»¬åœ¨å“ªé‡Œä½¿ç”¨äº† HuggingFace forwardï¼Ÿ

1. **Prefill é˜¶æ®µ**ï¼ˆç¬¬ 191 è¡Œï¼‰ï¼š
   ```python
   outputs = self.model(input_ids=seq_tokens, use_cache=True)
   ```
   - âœ… å¿…é¡»ä½¿ç”¨ï¼ˆéœ€è¦å®Œæ•´çš„æ¨¡å‹è®¡ç®—ï¼‰
   - âŒ ä½†åªèƒ½åˆ†åˆ«å¤„ç†æ¯ä¸ªåºåˆ—
   - âŒ æ— æ³•ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰ flattened tokens

2. **Decode é˜¶æ®µ**ï¼ˆç¬¬ 243 è¡Œï¼‰ï¼š
   ```python
   hidden_states = self.model.model.embed_tokens(token_tensor)
   ```
   - âœ… åªä½¿ç”¨ embedding å±‚
   - âœ… ç„¶åæ‰‹åŠ¨å¤„ç†æ¯ä¸€å±‚
   - âœ… ä½¿ç”¨ PagedAttention è®¡ç®— attention
   - âœ… ä¸å— HuggingFace é™åˆ¶

### ä¸ºä»€ä¹ˆè¯´"HuggingFace ä¸æ”¯æŒ ragged batching"ï¼Ÿ

**Prefill é˜¶æ®µçš„é—®é¢˜**ï¼š
- æˆ‘ä»¬**æƒ³**åšï¼šä¸€æ¬¡æ€§ä¼ å…¥ `[1, 109]`ï¼ˆ3 ä¸ªåºåˆ—çš„ flattened tokensï¼‰
- HuggingFace **åªèƒ½**åšï¼šåˆ†åˆ«ä¼ å…¥ `[1, 36]`, `[1, 37]`, `[1, 36]`ï¼ˆæ¯ä¸ªåºåˆ—å•ç‹¬å¤„ç†ï¼‰
- ç»“æœï¼šæ— æ³•å®ç°çœŸæ­£çš„ ragged batchingï¼ˆæ‰€æœ‰ tokens åœ¨ä¸€ä¸ª forward ä¸­å¤„ç†ï¼‰

**Decode é˜¶æ®µ**ï¼š
- æˆ‘ä»¬å·²ç»æ‰‹åŠ¨å¤„ç†ï¼Œä¸å—é™åˆ¶
- ä½†ä»ç„¶æ˜¯é¡ºåºå¤„ç†ï¼ˆPython loopï¼‰ï¼Œä¸æ˜¯çœŸæ­£çš„å¹¶è¡Œ

### å¦‚ä½•å®ç°çœŸæ­£çš„ ragged batchingï¼Ÿ

éœ€è¦ï¼š
1. **è‡ªå®šä¹‰ CUDA kernels**ï¼šå¤„ç† flattened tokens `[T]` + metadata
2. **ä¸ä½¿ç”¨ HuggingFace forward**ï¼šå®Œå…¨æ‰‹åŠ¨å®ç°æ¯ä¸€å±‚
3. **åŠ¨æ€ attention mask**ï¼šä» metadata æ„å»ºï¼ˆä½¿ç”¨ `seq_id_flat`ï¼‰

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ vLLM éœ€è¦é‡å†™æ•´ä¸ªæ¨¡å‹ forward é€»è¾‘çš„åŸå› ã€‚
