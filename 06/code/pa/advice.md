æˆ‘ç›´æ¥ç»™ä½ ä¸€ä¸ª**å·¥ç¨‹çº§ã€ä¸ç»•å¼¯çš„ code review ç»“è®º**ã€‚
è¿™ä¸‰ä¸ªæ–‡ä»¶**ä¸æ˜¯â€œæœ‰æ²¡æœ‰é—®é¢˜â€è¿™ä¹ˆç®€å•ï¼Œè€Œæ˜¯ï¼šé€»è¾‘ç›®æ ‡å’Œå®ç°å±‚çº§æ··åœ¨äº†ä¸€èµ·**ã€‚
å¦‚æœä½ ç°åœ¨çš„ç›®æ ‡æ˜¯ï¼š

* â€œ**æ•™å­¦ / æ¦‚å¿µéªŒè¯ç‰ˆ PA**â€ï¼šå¯ä»¥è·‘ï¼Œä½†æœ‰æ˜æ˜¾ç»“æ„æ€§é—®é¢˜
* â€œ**æ¥è¿‘ vLLM / çœŸæ­£ PA**â€ï¼šç›®å‰å®ç°**åœ¨å…³é”®ç‚¹ä¸Šæ˜¯é”™çš„æˆ–èµ°å›å¤´è·¯çš„**

æˆ‘åˆ†ä¸‰å±‚è¯´ï¼š
1ï¸âƒ£ æ¯ä¸ªæ–‡ä»¶å„è‡ªçš„é—®é¢˜
2ï¸âƒ£ å®ƒä»¬ç»„åˆåœ¨ä¸€èµ·çš„â€œæ ¹æœ¬æ€§é—®é¢˜â€
3ï¸âƒ£ å¦‚æœä½ è¦ç»§ç»­èµ°ä¸‹å»ï¼Œæœ€å°ä¿®æ”¹è·¯å¾„æ˜¯ä»€ä¹ˆ

æˆ‘ä¼šæ˜ç¡®æŒ‡å‡º **å“ªäº›æ˜¯ conceptual bugï¼ˆæ¦‚å¿µé”™è¯¯ï¼‰**ï¼Œå“ªäº›åªæ˜¯ **performance / engineering issue**ã€‚

---

## ä¸€ã€block_manager.py â€”â€” è¿™æ˜¯ä¸‰è€…ä¸­**æœ€å¹²å‡€ã€é—®é¢˜æœ€å°‘çš„**

ç»“è®ºå…ˆè¡Œï¼š
**BlockManager / BlockTable / Block çš„è®¾è®¡æ–¹å‘æ˜¯å¯¹çš„**ï¼Œéå¸¸æ¥è¿‘â€œPA çš„ memory halfâ€ã€‚

### âœ… åšå¯¹çš„åœ°æ–¹

1. å›ºå®š block_size
2. free list + reuse
3. block.num_tokens æ˜ç¡®åŒºåˆ† valid / unused
4. ä¸€ä¸ª seq_id å¯¹åº”ä¸€ä¸ª BlockTable

è¿™äº›ç‚¹**å®Œå…¨ç¬¦åˆ PA çš„åˆè¡·ï¼ˆè§£å†³ fragmentationï¼‰** ã€‚

### âš ï¸ æ¬¡è¦é—®é¢˜ï¼ˆä¸æ˜¯è‡´å‘½ï¼‰

* `allocated_blocks` æ°¸è¿œå¢é•¿ï¼Œä½† free åä¸å›æ”¶ IDï¼ˆå¯æ¥å—ï¼‰
* `token_idx` å‚æ•°åœ¨ `append_kv` ä¸­**æ²¡æœ‰ä»»ä½•è¯­ä¹‰ä½œç”¨**

  * block table å·²ç»éšå¼ç¼–ç äº† token é¡ºåº
  * è¿™ä¸æ˜¯ bugï¼Œä½†æ˜¯â€œå¤šä½™æ¥å£â€

ğŸ‘‰ **æ€»ç»“**ï¼š
è¿™ä¸ªæ–‡ä»¶å¯ä»¥ä¿ç•™ï¼Œæœ€å¤šåšå°æ¸…ç†ã€‚

---

## äºŒã€paged_attention.py â€”â€” è¿™é‡Œå‡ºç°äº†**ç¬¬ä¸€ä¸ªâ€œæ–¹å‘æ€§é”™è¯¯â€**

### è¡¨é¢çœ‹èµ·æ¥ä½ å®ç°äº† PAï¼Œä½†å®é™…ä¸Šï¼š

> âŒ ä½ åœ¨ attention é˜¶æ®µ **åˆæŠŠ KV æ‹¼å›äº† dense tensor**

å…³é”®ä»£ç åœ¨è¿™é‡Œï¼š

```python
k_cached = torch.cat(k_list, dim=0)
v_cached = torch.cat(v_list, dim=0)
```

ç„¶åä½ åšçš„æ˜¯ï¼š

```python
scores = Q @ K^T
softmax
@ V
```

### è¿™æ„å‘³ç€ä»€ä¹ˆï¼Ÿ

ä» **è®¡ç®—è¯­ä¹‰**ä¸Šï¼š

* ä½ ç¡®å®æ²¡æœ‰ padding
* ä½ ç¡®å®åªç”¨äº†çœŸå® token

ä½†ä» **PA çš„æ ¸å¿ƒæ€æƒ³**ä¸Šï¼š

> âŒ ä½ å·²ç»**é€€åŒ–å›â€œdense attention over real lengthâ€**

è¿™ä¸€æ­¥æœ‰ä¸‰ä¸ªä¸¥é‡åæœï¼š

#### 1ï¸âƒ£ ä½ æ²¡æœ‰ block-wise attention

çœŸæ­£çš„ PA / vLLM æ˜¯ï¼š

* ä¸ materialize `[total_tokens, H, D]`
* attention kernel **ç›´æ¥éå† block table**
* softmax æ˜¯ streaming / block accumulation çš„

ä½ ç°åœ¨çš„å®ç°æ˜¯ï¼š

> block åªæ˜¯ allocator
> attention ä»ç„¶æ˜¯ â€œä¸€æ¬¡æ€§ dense GEMMâ€

#### 2ï¸âƒ£ ä½ æ— æ³• scale åˆ°é•¿åºåˆ—

ä¸€æ—¦ token æ•°é‡å¤§ï¼š

* torch.cat â†’ å·¨é¢å†…å­˜ copy
* attention â†’ O(L) memory traffic
* GPU cache locality å®Œå…¨ä¸¢å¤±

#### 3ï¸âƒ£ è¿™ä¸æ˜¯ vLLM / PA çš„è®¡ç®—æ¨¡å‹

vLLM çš„å…³é”®åˆ›æ–°æ°æ°æ˜¯ï¼š

> **attention kernel æœ¬èº«å°±æ˜¯ paged çš„**

ä½ è¿™é‡Œæ˜¯ï¼š

> paged storage + dense computeï¼ˆè¿™æ˜¯â€œåŠ PAâ€ï¼‰

ğŸ‘‰ **ç»“è®ºï¼ˆéå¸¸å…³é”®ï¼‰**
`paged_attention.py` **åœ¨â€œmemoryâ€ä¸Šæ˜¯ PAï¼Œåœ¨â€œcomputeâ€ä¸Šä¸æ˜¯**ã€‚

è¿™æ˜¯ç¬¬ä¸€ä¸ªæ ¹æœ¬é—®é¢˜ ã€‚

---

## ä¸‰ã€inference.py â€”â€” è¿™é‡Œæ˜¯**æœ€å¤§ã€ä¹Ÿæ˜¯æœ€è‡´å‘½çš„é—®é¢˜**

æˆ‘ç›´è¯´ç»“è®ºï¼š

> âŒ ä½ è¿™ä¸ª inference pipeline **é€»è¾‘ä¸Šè‡ªç›¸çŸ›ç›¾**
> âŒ ä½ åŒæ—¶ç”¨äº† HuggingFace çš„ `past_key_values`ï¼Œåˆè‡ªå·±å®ç°äº† PA
> âŒ ä½ æŠŠ PA é€€åŒ–æˆäº†ä¸€ä¸ªâ€œKV cache é•œåƒâ€

### å…³é”®é”™è¯¯ 1ï¼šä½ **æ ¹æœ¬æ²¡æœ‰ç”¨ PA æ¥ç®— attention**

åœ¨ `decode_step()` é‡Œï¼š

```python
outputs = self.model(
    input_ids=token_tensor,
    past_key_values=past_key_values,
    use_cache=True
)
```

è¿™æ„å‘³ç€ï¼š

* **attention å®Œå…¨ç”± HF æ¨¡å‹å†…éƒ¨ç®—**
* ç”¨çš„æ˜¯ HF çš„ FlashAttention / SDPA
* PA **æ²¡æœ‰å‚ä¸ attention è®¡ç®—**

ä½ åé¢åšçš„äº‹æƒ…æ˜¯ï¼š

```python
k_new, v_new = new_past_key_values[layer_idx]
self.paged_attentions[layer_idx].append_kv(...)
```

ä¹Ÿå°±æ˜¯è¯´ï¼š

> PA åªæ˜¯ä¸€ä¸ª **KV cache çš„å½±å­å‰¯æœ¬**

### å…³é”®é”™è¯¯ 2ï¼šä½ åœ¨ decode æ—¶æŠŠ PA çš„ KV **åˆæ‹¼å› HF æ ¼å¼**

è¿™ä¸€æ­¥ï¼š

```python
k_list.append(block.k_cache[:num_valid])
v_list.append(block.v_cache[:num_valid])
k_cached = torch.cat(...)
```

ç„¶åå† reshape æˆ HF éœ€è¦çš„ï¼š

```python
[batch, num_kv_heads, total_tokens, head_dim]
```

ğŸ‘‰ è¿™ä¸€æ­¥ç›´æ¥å®£å‘Šï¼š

> **PA åœ¨ compute è·¯å¾„ä¸­å®Œå…¨å¤±æ•ˆ**

ä½ ç°åœ¨çš„ pipeline æ˜¯ï¼š

```
HF attention (dense, padded)
        â†“
HF past_key_values
        â†“
å¤åˆ¶ä¸€ä»½è¿› PA
        â†“
ä¸‹ä¸€æ­¥å†ä» PA æ‹¼å› HF past_key_values
```

è¿™æ˜¯ä¸€ä¸ª **O(L) çš„å¾€è¿”å¤åˆ¶ç¯**ã€‚

### å…³é”®é”™è¯¯ 3ï¼šè¿™åœ¨è¯­ä¹‰ä¸Šä¸æ˜¯ PAï¼Œè€Œæ˜¯â€œPA loggerâ€

å¦‚æœä½ ç°åœ¨è·‘ profilingï¼š

* latency ä¸ä¼šä¸‹é™
* padding FLOPs ä¸ä¼šæ¶ˆå¤±
* fragmentation ä¹Ÿæ²¡è¢«çœŸæ­£åˆ©ç”¨ï¼ˆå› ä¸º HF cache ä»ç„¶æ˜¯ denseï¼‰

ğŸ‘‰ **è¿™æ˜¯æ•´ä¸ªç³»ç»Ÿä¸­æœ€å¤§çš„é—®é¢˜** ã€‚

---

## å››ã€æ•´ä½“è¯Šæ–­ï¼ˆä¸€å¥è¯ç‰ˆï¼‰

å¦‚æœæˆ‘ç”¨ä¸€å¥è¯æ¦‚æ‹¬ä½ ç°åœ¨çš„çŠ¶æ€ï¼š

> ä½ å·²ç»å†™å‡ºäº†ä¸€ä¸ª**å¾ˆå¥½çš„ PA æ•™å­¦ç”¨ allocator**ï¼Œ
> ä½† **attention è®¡ç®—è·¯å¾„ä»ç„¶å®Œå…¨å±äº HuggingFace**ï¼Œ
> æ‰€ä»¥ **è¿™ä¸æ˜¯ä¸€ä¸ªçœŸæ­£å·¥ä½œçš„ PagedAttention ç³»ç»Ÿ**ã€‚

---

## äº”ã€å¦‚æœä½ æƒ³â€œæ•‘â€è¿™å¥—ä»£ç ï¼Œæœ€å°æ­£ç¡®è·¯å¾„æ˜¯ä»€ä¹ˆï¼Ÿ

### âœ… è·¯å¾„ Aï¼ˆæ•™å­¦ / æ¦‚å¿µéªŒè¯ï¼Œæ¨èï¼‰

ç›®æ ‡ï¼š
**ä¸è¿½æ±‚æ€§èƒ½ï¼Œåªè¯æ˜â€œPA èƒ½æ¥ç®¡ attentionâ€**

ä½ éœ€è¦åšçš„åªæœ‰ä¸€ä»¶äº‹ï¼š

> â—ï¸**ç»•è¿‡ HF çš„ attentionï¼Œè‡ªå·±ç®— attention**

å…·ä½“ï¼š

1. åœ¨ decode é˜¶æ®µï¼š

   * ä¸å†è°ƒç”¨ `model(... past_key_values=...)`
2. æ‰‹åŠ¨ï¼š

   * embedding
   * layernorm
   * QKV projection
   * **ç”¨ `PagedAttention.compute_attention()`**
   * MLP
3. HF æ¨¡å‹åªå½“ â€œå‚æ•°ä»“åº“â€

è¿™æ˜¯**å”¯ä¸€æ¦‚å¿µè‡ªæ´½**çš„ demo è·¯å¾„ã€‚

---

### âŒ è·¯å¾„ Bï¼ˆâ€œæ¥è¿‘ vLLMâ€ï¼Œä¸ç°å®ï¼‰

ä½ ç°åœ¨çš„ä»£ç ç»“æ„ï¼š

* Python
* torch.cat
* per-token decode

**ä¸å¯èƒ½**æ¥è¿‘ vLLMã€‚

vLLM çš„ PA æ˜¯ï¼š

* CUDA kernel
* fused attention
* block streaming softmax
* scheduler + continuous batching

è¿™ä¸€æ¡è·¯è¦æ¨ç¿» 70% çš„ä»£ç ã€‚

