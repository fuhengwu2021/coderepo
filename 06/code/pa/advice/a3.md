Yes. With HuggingFace’s Qwen2Attention, RoPE is not “hidden”; it is an actual submodule on the attention layer plus a public helper that applies the rotation.

Concretely, in Qwen2Attention.forward the flow is:

* self.rotary_emb(...) produces (cos, sin)
* apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids) applies RoPE to q and k
* then caching and attention proceed

You can see this directly in the reference implementation: Qwen2Attention defines self.rotary_emb and calls it to get cos/sin, then calls apply_rotary_pos_emb. ([Hugging Face][1])

1. How to access “internal RoPE” objects

* The RoPE module is at:
  model.model.layers[layer_idx].self_attn.rotary_emb

* Its forward signature is effectively:
  rotary_emb(x, seq_len) -> (cos[0:seq_len], sin[0:seq_len]) ([Hugging Face][1])

So you can reuse it exactly the same way Qwen2Attention does.

2. Minimal code to reuse RoPE (cos/sin + apply) for Qwen2Attention

You’ll typically want this inside your PA-attention replacement, right after you’ve formed query_states/key_states (already shaped to [B, Hq/Hkv, q_len, D]):

```latex
import torch

# Import helpers from the same module that defines Qwen2Attention
from transformers.models.qwen2.modeling_qwen2 import apply_rotary_pos_emb

def rope_qk_from_qwen2attention(attn, query_states, key_states, value_states, position_ids, kv_seq_len):
    # attn is an instance of Qwen2Attention (e.g., model.model.layers[i].self_attn)
    # query_states: [B, Hq, q_len, D]
    # key_states:   [B, Hkv, q_len, D]
    # value_states: [B, Hkv, q_len, D]
    # position_ids: [B, q_len]
    # kv_seq_len: total KV length after considering cache, used to size RoPE cache

    cos, sin = attn.rotary_emb(value_states, seq_len=kv_seq_len)
    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)
    return query_states, key_states, cos, sin
```

This mirrors the upstream logic: cos/sin from attn.rotary_emb and rotation via apply_rotary_pos_emb. ([Hugging Face][1])

3. Notes that matter for prefill+decode + PA

A) Decode position length (kv_seq_len / rotary_seq_len)
In decode, RoPE cache length must cover the current “absolute position”. In the flash-attn path, they sometimes compute rotary_seq_len = max(kv_seq_len, max(position_ids)+1) to account for padding/offset positions. ([Hugging Face][1])
If you do continuous batching / cache_position offsets, follow the same rule: make sure seq_len passed to rotary_emb is at least max(position_ids)+1.

B) GQA
Do not call repeat_kv if your PA stores KV in Hkv heads. In the reference, repeat_kv is used to expand KV from Hkv to Hq for the dense attention matmul. ([Hugging Face][1])
For PA, keep KV as Hkv and map q heads to kv heads inside your PA kernel.

C) Integrating into your PA path
Inside each layer during prefill/decode:

* Compute q, k, v with the model’s projections
* Apply RoPE using the layer’s attn.rotary_emb + apply_rotary_pos_emb
* Append k/v (Hkv) into your paged KV blocks
* Compute attention output using your PA (using q in Hq and the GQA mapping)

If you tell me your transformers version (the exact pip version string), I can point you to the exact import path for apply_rotary_pos_emb in your install (the name is stable, but module paths do shift across versions).

[1]: https://huggingface.co/Qwen/Qwen2.5-Math-PRM-72B/blob/main/modeling_qwen2_rm.py "modeling_qwen2_rm.py · Qwen/Qwen2.5-Math-PRM-72B at main"
