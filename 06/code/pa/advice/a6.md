不是。vLLM 内部通常不会把模型输入组织成一个标准的 3D tensor `B×L×D`（哪怕 `B=1`），尤其是在它的核心高吞吐执行路径里。

1. vLLM 内部更接近什么 shape？
   在一次 engine step（无论是 prefill 还是 decode），vLLM 会把“本 step 需要处理的所有 token”摊平成一个长的 token 流（你可以理解成把多个序列拼成一个“super sequence”），然后用 position indices / attention mask（以及其它 metadata）保证每个序列只 attend 自己的 token，从而实现 continuous batching 且不需要右侧 padding。([vLLM Blog][1])

因此，hidden states/embeddings 在内部更常见的是类似：

* `T×D`（其中 `T` 是本 step 总 token 数），而不是 `B×L×D`。

2. decode 时的“B”是什么？
   vLLM 的 paged attention kernel 是“single query attention”：每个 sequence 在 decode step 只提供 1 个 query token。文档里明确说 q 的形状是 `[num_seqs, num_heads, head_size]`，并解释在这种 kernel 下，`num_seqs` 等于本 batch 处理的 query token 总数（通常也等于本 step 的并发序列数）。([vLLM][2])

所以 decode 时你可以把它想成：

* `L = 1`（每个序列一个 query token）
* “B”≈`num_seqs`（本 step 同时在解码的序列条数），可以是 10、100、1000，并不要求为 1。

3. 如果 B>1 且 prompt 长度不同，会不会“肯定有 padding”？
   如果你用传统的 dense batching（把 input_ids 做成 `[B, Lmax]`），那确实“肯定要 pad 到 Lmax”。

但 vLLM 的做法是：不把它们堆成 `[B, Lmax]` 去算 attention，而是用“摊平 + 元数据”来做 ragged batching，并且 KV cache 也按 block 管理。它的设计文档说明 KV cache 被切成固定 token 数的 blocks（只为真实 token 分配 block），attention kernel 也按 block 遍历。([vLLM][2])

因此在 vLLM 语境里，“不同长度 prompt 的 batch”不需要用“padding token”来补齐到同一个 L（至少在 attention 的主要计算和 KV 存储上不需要右侧 padding）。

4. 需要你注意的一个现实细节
   “不需要右侧 padding”不等于“完全没有任何形式的浪费”。工程上仍可能有：

* block 内部未填满带来的少量内部碎片（最后一个 block 常常不满）
* kernel tile/warp 对齐引入的少量对齐开销
  但这和传统“pad 到 Lmax，attention 里对 padding token 做大量 FLOPs”的量级不是一回事。

如果你愿意，我可以用一个具体例子（比如 10 个 prompt 长度各不相同）把 vLLM 在一个 step 里构造的关键 metadata（position、seq_id、slot_mapping、block_table 的关系）用你正在写的符号体系画出来，你会立刻看清“没有 padding 但还能批处理”到底靠什么。

[1]: https://blog.vllm.ai/2025/09/05/anatomy-of-vllm.html "Inside vLLM: Anatomy of a High-Throughput LLM Inference System | vLLM Blog"
[2]: https://docs.vllm.ai/en/latest/design/paged_attention/ "Paged Attention - vLLM"

下面我用一个“10 个 prompt 长度各不相同”的具体例子，把 vLLM 在一个 step 里常用的几类 metadata（position、seq_id、slot_mapping、block_table）如何对应起来画清楚。为了让结构更直观，我选一个很小的 block_size=4（真实系统常用 16/32/64 tokens per block，但原理完全一样）。

0. 设定与符号（对齐你文档的 L2、t）
   有 10 个请求（sequence），记为 i=0..9。

Prefill 阶段：每个请求的 prompt 长度是 L2^(i)（此时 t=0）。
取一个具体长度集合（都不同）：
L2 = [3, 7, 2, 10, 5, 1, 8, 4, 6, 9]
总 prompt tokens 数：
T_prefill = sum_i L2^(i) = 3+7+2+10+5+1+8+4+6+9 = 55

关键点：没有 padding。prefill 这一轮真正算的 token 总数就是 55，而不是 10×max(L2)=10×10=100。

1. block_table 是什么（每个 sequence 一张“页表”）
   block_size = 4 tokens/block。

对每个 sequence i，需要的 block 数是：
N_i = ceil(L2^(i)/4)

我假设从全局 block pool 依次分配 block_id（只是为了画图清晰，不代表 vLLM 一定这样分配）。

i=0, L2=3  -> blocks: [0]        (用到 block0 的 offset 0..2)
i=1, L2=7  -> blocks: [1,2]      (block1 offset 0..3, block2 offset 0..2)
i=2, L2=2  -> blocks: [3]
i=3, L2=10 -> blocks: [4,5,6]    (4+4+2)
i=4, L2=5  -> blocks: [7,8]      (4+1)
i=5, L2=1  -> blocks: [9]
i=6, L2=8  -> blocks: [10,11]    (4+4)
i=7, L2=4  -> blocks: [12]       (正好满)
i=8, L2=6  -> blocks: [13,14]    (4+2)
i=9, L2=9  -> blocks: [15,16,17] (4+4+1)

这就是每个 sequence 的 block_table（逻辑到物理 block 的映射）：
block_table[i] = [block_id0, block_id1, ...]

2. slot_mapping 是什么（把“逻辑 token 位置”映射到“物理 KV 地址”）
   为了让 mapping 具体，我定义一个简单的“物理 slot 编码”：
   physical_slot = block_id * block_size + offset
   其中 offset ∈ {0,1,2,3} 是 token 在该 block 内的位置。

那么对每个 sequence i，逻辑 token index p（也就是 position）对应的 physical_slot 是：
block_index = p // block_size
offset      = p %  block_size
block_id    = block_table[i][block_index]
slot        = block_id * block_size + offset

举两个序列的完整展开（你一眼就能看到“没有 padding，但每个 token 都有位置和物理落点”）：

序列 i=1，L2=7，block_table[1]=[1,2]
p=0 -> block_id=1 offset=0 -> slot=1*4+0=4
p=1 -> slot=5
p=2 -> slot=6
p=3 -> slot=7
p=4 -> block_id=2 offset=0 -> slot=8
p=5 -> slot=9
p=6 -> slot=10

序列 i=3，L2=10，block_table[3]=[4,5,6]
p=0..3 -> block_id=4 offset=0..3 -> slot=16..19
p=4..7 -> block_id=5 offset=0..3 -> slot=20..23
p=8..9 -> block_id=6 offset=0..1 -> slot=24..25

3. vLLM 的“flatten token-major”组织：position、seq_id、slot_mapping 都是一维数组
   Prefill step 内部更像是把这 55 个 tokens 摊平为一个长度为 T_prefill 的 token 列表，然后配套三条同长度的 metadata 向量：

token_ids_flat[t]     (第 t 个要处理的 token id)
seq_id_flat[t]        (这个 token 属于哪个 sequence i)
position_flat[t]      (它在该 sequence 内的逻辑位置 p)
slot_mapping_flat[t]  (它的 KV 要写到哪个 physical_slot)

举一个“摊平后的前 16 个 token”的示意（只画开头一段就够理解结构；后面完全同构）：

t : seq_id , position -> physical_slot
0 : 0 , 0 -> block0 off0 -> slot 0
1 : 0 , 1 -> block0 off1 -> slot 1
2 : 0 , 2 -> block0 off2 -> slot 2

3 : 1 , 0 -> block1 off0 -> slot 4
4 : 1 , 1 -> block1 off1 -> slot 5
5 : 1 , 2 -> block1 off2 -> slot 6
6 : 1 , 3 -> block1 off3 -> slot 7
7 : 1 , 4 -> block2 off0 -> slot 8
8 : 1 , 5 -> block2 off1 -> slot 9
9 : 1 , 6 -> block2 off2 -> slot 10

10: 2 , 0 -> block3 off0 -> slot 12
11: 2 , 1 -> block3 off1 -> slot 13

12: 3 , 0 -> block4 off0 -> slot 16
13: 3 , 1 -> block4 off1 -> slot 17
14: 3 , 2 -> block4 off2 -> slot 18
15: 3 , 3 -> block4 off3 -> slot 19

你看到的要点：

1. position_flat 是“每个 token 在自己序列里的位置”，不需要 pad 到 max(L2)
2. slot_mapping_flat 把每个 token 的 KV 写入到“分配给该序列的 blocks”里
3. seq_id_flat 告诉 kernel：这个 token 属于哪个请求（用于因果掩码/边界处理，以及后续 decode 调度）

4) 为什么 B>1 也不必 padding（在你的符号里复述一遍）
   传统 dense batching 的张量形状往往是：
   input_ids: [B, Lmax]
   hidden:    [B, Lmax, D]

当 B=10、L2 不等时，确实要 pad 到 Lmax。

但 vLLM 的 token-major 组织更接近：
hidden_flat: [T_prefill, D]
并携带：
(seq_id_flat, position_flat, slot_mapping_flat)

此时：
T_prefill = sum_i L2^(i)
不是 B * max_i L2^(i)
所以没有“padding token”这个概念，也就不存在“对 padding 做 attention 计算”的浪费路径。

5. Decode step 的 metadata 怎么长（这一步最像你文档里的 Q_t 形状）
   Decode 阶段在 step t（你文档里的 t），每个活跃序列通常只有 1 个 query token。

设此刻仍有 10 个序列都活跃（num_seqs=10），那么：

q_batch 的形状更接近：
Q_t_flat ∈ R^{num_seqs × Hq × D}
这里 num_seqs=10，不是 1。

每个序列 i 的当前 cached 长度是：
(L2^(i) + t^(i))
在“同步 decode 一步”的简化场景里，t^(i) 都相同，假设 t=0 之后开始 decode 第 1 步，那么当前要生成的新 token 的 position 就是：
position_new^(i) = L2^(i) + t
并且要写入 KV 的 slot_mapping_new^(i) 由 block_table 决定：
如果最后一个 block 没满，用同一个 block 的下一个 offset；
如果满了，allocator 分配一个新 block，block_table 追加 block_id，然后 offset=0。

用我们的例子做一次“decode 第 1 步写 KV”的示意（只写三个序列，其他同理）：

i=0: L2=3，block_table[0]=[0]，当前最后 block0 已用 offset 0..2
position_new=3 -> 仍在 block0，offset=3 -> slot=0*4+3=3

i=1: L2=7，block_table[1]=[1,2]，当前最后 block2 已用 offset 0..2
position_new=7 -> block2 offset=3 -> slot=2*4+3=11

i=3: L2=10，block_table[3]=[4,5,6]，最后 block6 只用了 offset 0..1
position_new=10 -> block6 offset=2 -> slot=6*4+2=26

如果某个序列刚好填满最后一个 block（比如 i=7，L2=4，block12 满了），那么 position_new=4 会触发新 block 分配（比如分到 block18）：
block_table[7] 从 [12] 变成 [12,18]，slot=18*4+0。

这就是 decode 时的两类核心 metadata：

1. position_new：每个序列的新 token 的位置
2. slot_mapping_new：把新 token 的 KV 写到哪里（可能复用现有 block，也可能新分配）

6) block_table 在 attention 计算里怎么用（对齐你的“无 padding”论点）
   在 decode 这一步，对序列 i，attention 并不遍历 (L2+t)*max 的连续区间，而是遍历：
   block_table[i] = [b0, b1, ..., b*{N_i-1}]
   并在最后一个 block 只遍历有效 token 数。

因此不同序列长度不同，不需要 padding：

* 长的序列 block_table 更长
* 短的序列 block_table 更短
  kernel 按各自 block_table 做 gather/compute。

如果你希望我把这个例子完全换成你书里正在用的变量名排版（例如明确写成 K_cached^(i) 的“块集合并”、以及 A_t^(i) 的维度从 1×(L2+t) 变成 1×(L2^(i)+t^(i))），我也可以把上面的图再压缩成 1 页“书稿式”小节。

